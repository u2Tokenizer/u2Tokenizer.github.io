<!DOCTYPE html>
<html lang="zh" data-theme="dark">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>‰∏≠Ëã±Êñá Tab ÂàáÊç¢Á§∫‰æã</title>
    <script>
        tailwind = {}
        tailwind.config = {
            theme: {
                extend: {},
            },
            plugins: [window.daisyui],
            daisyui: {
                themes: ["dark", "light"],
            },
        }
    </script>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdn.jsdelivr.net/npm/daisyui@4.7.2/dist/full.css" rel="stylesheet" type="text/css" />
    <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
</head>

<body class="min-h-screen flex flex-col items-center">
    <button id="theme-toggle" onclick="toggleTheme()" class="absolute right-0 top-0 mt-2 mr-2 btn btn-sm">Ô∏èÔ∏èÔ∏èÔ∏èÔ∏èÔ∏è‚òÄ</button>
    <div class="text-center max-w-3xl px-4">
        <h1 class="text-3xl font-bold my-8"><var>&micro<sup>2</sup></var>Tokenizer: Differentiable Multi-Scale
            Multi-Modal Tokenizer for Radiology Report Generation</h1>
        <div class="flex flex-wrap justify-center gap-2 text-lg mb-2">
            <span>Siyou Li<sup>1</sup>, </span>
            <span>Pengyao Qin<sup>2</sup>, </span>
            <span>Huanan Wu<sup>3</sup>, </span>
            <span>Dong Nie<sup>4</sup>, </span>
            <span>Arun J. Thirunavukarasu<sup>5</sup>, </span>
            <span>Juntao Yu<sup>1</sup>, </span>
            <span>Le Zhang<sup>2, 6</sup></span>
        </div>
        <div class="text-base text-base-content/80">
            <p><sup>1</sup>School of Electronic Engineering and Computer Science,</p>
            <p>Queen Mary University of London, London, UK</p>
            <p><sup>2</sup>School of Engineering, College of Engineering and Physical Sciences,</p>
            <p>University of Birmingham, Birmingham, UK</p>
            <p><sup>3</sup>Guangdong University of Technology, Guangdong, China</p>
            <p><sup>4</sup>Meta Inc. US</p>
            <p><sup>5</sup>Nuffield Department of Clinical Neurosciences,</p>
            <p>University of Oxford, Oxford, UK</p>
            <p><sup>6</sup>William Harvey Research Institute, NIHR Barts Biomedical Research Centre,</p>
            <p>Queen Mary University London, London, UK</p>
        </div>
    </div>
    
    <p>Our proposed model, <strong>Œº¬≤LLM</strong>, leverages a multi-scale, multi-modal architecture to generate accurate and clinically salient radiology reports from CT scans.</p>
<hr>
<h2 id="-overview">üìù Overview</h2>
<p>[cite_start]Automated radiology report generation (RRG) is a promising application of AI that aims to assist radiologists by producing detailed textual reports from medical images like CT scans[cite: 2]. However, this task faces two major hurdles:</p>
<ol>
<li>[cite_start]<strong>Complex Information Extraction</strong>: Efficiently extracting all relevant diagnostic information from large, variable-resolution imaging data is computationally intensive and prone to information loss[cite: 3, 19, 23].</li>
<li>[cite_start]<strong>Subjective Evaluation</strong>: Traditional text-generation metrics often fail to capture the clinical accuracy and semantic meaning that are crucial for medical reports[cite: 3, 22, 24].</li>
</ol>
<p>To address these challenges, we introduce <strong>Œº¬≤LLM</strong>, a multi-scale multimodal large language model. [cite_start]At its core is the novel <strong>Œº¬≤ Tokenizer</strong>, an intermediate layer that intelligently fuses visual features from CT scans with textual information[cite: 4, 5]. [cite_start]The model is further refined using Direct Preference Optimization (DPO), guided by the specialized medical report evaluation metric, GREEN, to ensure the generated reports align with expert standards[cite: 5, 29].</p>
<p>[cite_start]Our experimental results on four large-scale CT datasets show that Œº¬≤LLM outperforms existing methods, highlighting its potential for generating high-quality radiology reports even with limited training data[cite: 6].</p>
<hr>
<h2 id="-key-contributions">‚ú® Key Contributions</h2>
<ul>
<li>[cite_start]<strong>Œº¬≤LLM Framework</strong>: We propose a novel multi-modal large language model (MLLM) designed to efficiently preserve critical details from medical imaging by integrating guided questions[cite: 25].</li>
<li>[cite_start]<strong>Œº¬≤ Tokenizer Layer</strong>: The core of our framework is the Œº¬≤ Tokenizer, an intermediate layer that uses multi-level attention and multi-scale aggregation to refine and fuse visual and text embeddings, maximizing semantic correspondence while maintaining computational efficiency[cite: 26, 27].</li>
<li>[cite_start]<strong>Enhanced Training with DPO</strong>: We employ Direct Preference Optimization (DPO) to align our model&#39;s outputs with expert-validated clinical accuracy[cite: 29]. [cite_start]The preference data is curated using the GREEN score, a robust LLM-based metric for evaluating the clinical accuracy of radiology reports[cite: 72, 75, 76].</li>
<li>[cite_start]<strong>State-of-the-Art Performance</strong>: Despite its smaller parameter size (1B), our model consistently outperforms larger baseline models (7B to 14B) across multiple datasets, demonstrating the effectiveness of our approach[cite: 109, 110, 111].</li>
</ul>
<hr>
<h2 id="-model-architecture">üèóÔ∏è Model Architecture</h2>
<p>The Œº¬≤LLM framework integrates a 3D Vision Transformer (ViT3D) as the image encoder and a large language model (LLM) for report generation. The key innovation, the <strong>Œº¬≤ Tokenizer</strong>, acts as a bridge between them.</p>
<p><strong>Overall Pipeline (<code>Œº¬≤LLM</code>)</strong>
<em>Fig. [cite_start]1: Overview of our proposed ¬µ¬≤LLM model that is centered with the Œº¬≤ Tokenizer layer for high quality RRG task.</em> [cite: 30]</p>
<ol>
<li><strong>Input</strong>: A 3D CT scan and a related textual question (e.g., &quot;Can you provide a diagnosis based on the findings in the abdomen?&quot;).</li>
<li><strong>Image Encoder (ViT3D)</strong>: The CT scan is split into multiple frames to avoid information loss from downsampling. [cite_start]The ViT3D processes these frames to extract a sequence of visual tokens[cite: 35, 36, 37].</li>
<li><strong>Œº¬≤ Tokenizer</strong>: This layer takes the visual tokens from the encoder and the tokenized question. [cite_start]It uses multi-scale attention mechanisms to fuse the two modalities, producing a compact and information-rich set of embeddings[cite: 40, 41].</li>
<li>[cite_start]<strong>LLM</strong>: The final image embeddings are passed to the LLM along with the original question to generate the detailed radiology report[cite: 42, 43, 46].</li>
</ol>
<h3 id="the-tokenizer-module">The Œº¬≤ Tokenizer Module</h3>
<p>[cite_start]The Œº¬≤ Tokenizer is built upon the Linear Video Tokenizer (LinVT) [cite: 48] and introduces three key improvements:</p>
<p><em>Fig. [cite_start]2: The illustration of our proposed Œº¬≤ Tokenizer.</em> [cite: 44]</p>
<ul>
<li><strong>Relative Positional Encoding (RPE)</strong>: Instead of absolute positions, we use relative positional encodings within the attention mechanism. [cite_start]This allows the model to better capture local relationships between different areas of the 3D scan, which is crucial for identifying anatomical patterns[cite: 52].</li>
<li>[cite_start]<strong>Differentiable Token Selection (DTS)</strong>: Traditional &quot;hard&quot; top-k token selection can lead to information loss and slow optimization[cite: 53]. We replace it with a fully differentiable &quot;soft&quot; selection, which computes a weighted sum of all tokens. [cite_start]This preserves more information and allows gradients to flow back to all visual tokens, improving training stability[cite: 57, 58, 59].</li>
<li>[cite_start]<strong>Dynamic Multi-scale Pooling (DMTP)</strong>: Rather than using fixed pooling kernel sizes, our dynamic approach allows the network to learn how to weight and select the most appropriate pooling strategy based on the input, making the feature extraction process more adaptive and effective[cite: 63, 64, 65, 68].</li>
</ul>
<hr>
<h2 id="-results">üìä Results</h2>
<p>Our model was evaluated on several benchmark datasets against various high-performing LLMs. [cite_start]We used both traditional metrics (ROUGE, METEOR, BERTScore) and the advanced LLM-based <strong>GREEN</strong> score, which measures clinical accuracy[cite: 99, 100, 101].</p>
<h3 id="performance-comparison">Performance Comparison</h3>
<p>[cite_start]Œº¬≤LLM achieves state-of-the-art results across all datasets, significantly outperforming larger models like LaMed-Llama-2-7B and RadFM-14B[cite: 111]. [cite_start]The use of DPO fine-tuned with GREEN scores further boosted performance, with a notable <strong>20% improvement</strong> in the GREEN score on average[cite: 114, 115].</p>
<p>[cite_start]<strong>Table 1: Performance Comparison Across Different Datasets</strong> [cite: 97]</p>
<table>
<thead>
<tr>
<th style="text-align:left">Datasets</th>
<th style="text-align:left">Models</th>
<th style="text-align:center">ROUGE-1</th>
<th style="text-align:center">GREEN</th>
<th style="text-align:center">METEOR</th>
<th style="text-align:center">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LaMed-Phi-3-4B</td>
<td style="text-align:center">0.136</td>
<td style="text-align:center">0.011</td>
<td style="text-align:center">0.058</td>
<td style="text-align:center">0.807</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LaMed-Llama-2-7B</td>
<td style="text-align:center">0.139</td>
<td style="text-align:center">0.009</td>
<td style="text-align:center">0.060</td>
<td style="text-align:center">0.810</td>
</tr>
<tr>
<td style="text-align:left"><strong>Abdomen Atlas</strong></td>
<td style="text-align:left">RadFM-14B</td>
<td style="text-align:center">0.037</td>
<td style="text-align:center">0.000</td>
<td style="text-align:center">0.013</td>
<td style="text-align:center">0.794</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RadGPT-N</td>
<td style="text-align:center">0.247</td>
<td style="text-align:center"></td>
<td style="text-align:center">0.112</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT)</strong></td>
<td style="text-align:center"><strong>0.529</strong></td>
<td style="text-align:center"><strong>0.281</strong></td>
<td style="text-align:center"><strong>0.295</strong></td>
<td style="text-align:center"><strong>0.891</strong></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT\&amp;DPO)</strong></td>
<td style="text-align:center"><strong>0.567</strong></td>
<td style="text-align:center"><strong>0.346</strong></td>
<td style="text-align:center"><strong>0.319</strong></td>
<td style="text-align:center"><strong>0.895</strong></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LaMed-Phi-3-4B</td>
<td style="text-align:center">0.130</td>
<td style="text-align:center">0.002</td>
<td style="text-align:center">0.050</td>
<td style="text-align:center">0.814</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LaMed-Llama-2-7B</td>
<td style="text-align:center">0.103</td>
<td style="text-align:center">0.001</td>
<td style="text-align:center">0.048</td>
<td style="text-align:center">0.815</td>
</tr>
<tr>
<td style="text-align:left"><strong>CT-Rate</strong></td>
<td style="text-align:left">RadFM-14B</td>
<td style="text-align:center">0.054</td>
<td style="text-align:center">0.014</td>
<td style="text-align:center">0.017</td>
<td style="text-align:center">0.812</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CT-CHAT-8B</td>
<td style="text-align:center">0.294</td>
<td style="text-align:center">0.113</td>
<td style="text-align:center">0.221</td>
<td style="text-align:center">0.815</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT)</strong></td>
<td style="text-align:center"><strong>0.517</strong></td>
<td style="text-align:center"><strong>0.384</strong></td>
<td style="text-align:center"><strong>0.330</strong></td>
<td style="text-align:center"><strong>0.879</strong></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT\&amp;DPO)</strong></td>
<td style="text-align:center"><strong>0.539</strong></td>
<td style="text-align:center"><strong>0.429</strong></td>
<td style="text-align:center"><strong>0.359</strong></td>
<td style="text-align:center"><strong>0.890</strong></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LaMed-Phi-3-4B</td>
<td style="text-align:center">0.126</td>
<td style="text-align:center">0.009</td>
<td style="text-align:center">0.047</td>
<td style="text-align:center">0.821</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LaMed-Llama-2-7B</td>
<td style="text-align:center">0.163</td>
<td style="text-align:center">0.009</td>
<td style="text-align:center">0.065</td>
<td style="text-align:center">0.823</td>
</tr>
<tr>
<td style="text-align:left"><strong>AMOS-MM</strong></td>
<td style="text-align:left">RadFM-14B</td>
<td style="text-align:center">0.046</td>
<td style="text-align:center">0.001</td>
<td style="text-align:center">0.015</td>
<td style="text-align:center">0.812</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT)</strong></td>
<td style="text-align:center"><strong>0.421</strong></td>
<td style="text-align:center"><strong>0.339</strong></td>
<td style="text-align:center"><strong>0.249</strong></td>
<td style="text-align:center"><strong>0.881</strong></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT\&amp;DPO)</strong></td>
<td style="text-align:center"><strong>0.459</strong></td>
<td style="text-align:center"><strong>0.400</strong></td>
<td style="text-align:center"><strong>0.876</strong></td>
<td style="text-align:center"><strong>0.881</strong></td>
</tr>
</tbody>
</table>
<h3 id="ablation-study">Ablation Study</h3>
<p>[cite_start]Ablation experiments confirmed that each component of the Œº¬≤ Tokenizer contributes positively to the model&#39;s performance[cite: 117]. [cite_start]Differentiable Token Selection (DTS) provided the most significant boost, improving the GREEN score by up to 0.2 points[cite: 118].</p>
<p>[cite_start]<strong>Table 2: Ablation Study on Œº¬≤ Tokenizer Components</strong> [cite: 104]</p>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:center">BLEU</th>
<th style="text-align:center">ROUGE-1</th>
<th style="text-align:center">GREEN</th>
<th style="text-align:center">METEOR</th>
<th style="text-align:center">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Baseline</td>
<td style="text-align:center">0.190</td>
<td style="text-align:center">0.405</td>
<td style="text-align:center">0.204</td>
<td style="text-align:center">0.210</td>
<td style="text-align:center">0.864</td>
</tr>
<tr>
<td style="text-align:left">+RPE</td>
<td style="text-align:center">0.281</td>
<td style="text-align:center">0.421</td>
<td style="text-align:center">0.277</td>
<td style="text-align:center">0.236</td>
<td style="text-align:center">0.880</td>
</tr>
<tr>
<td style="text-align:left">+DTS</td>
<td style="text-align:center">0.271</td>
<td style="text-align:center">0.411</td>
<td style="text-align:center">0.299</td>
<td style="text-align:center">0.240</td>
<td style="text-align:center">0.888</td>
</tr>
<tr>
<td style="text-align:left">+DMTP</td>
<td style="text-align:center">0.254</td>
<td style="text-align:center">0.401</td>
<td style="text-align:center">0.233</td>
<td style="text-align:center">0.220</td>
<td style="text-align:center">0.874</td>
</tr>
<tr>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT)</strong></td>
<td style="text-align:center">0.279</td>
<td style="text-align:center">0.421</td>
<td style="text-align:center">0.339</td>
<td style="text-align:center">0.249</td>
<td style="text-align:center">0.881</td>
</tr>
<tr>
<td style="text-align:left"><strong>Œº¬≤LLM-1B (SFT\&amp;DPO)</strong></td>
<td style="text-align:center"><strong>0.336</strong></td>
<td style="text-align:center"><strong>0.459</strong></td>
<td style="text-align:center"><strong>0.400</strong></td>
<td style="text-align:center"><strong>0.876</strong></td>
<td style="text-align:center"><strong>0.881</strong></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="-example-output">üñºÔ∏è Example Output</h2>
<p>Below is an example of a report generated by our model. [cite_start]The heat map on the left visualizes the model&#39;s cross-attention scores, indicating which regions of the CT scan it focused on to answer the question[cite: 120].</p>
<p><em>Fig. [cite_start]4: An example of the generated report from our ¬µ¬≤LLM-1B (SFT\&amp;DPO).</em> [cite: 108]</p>
<p>[cite_start]The generated text demonstrates high clinical coherence, providing a structured and accurate interpretation of the CT findings, including descriptions of liver parenchyma density, gallbladder morphology, and other critical observations[cite: 122, 123, 124].</p>
<hr>
<h2 id="-datasets">üíø Datasets</h2>
<p>The models were trained and evaluated on the following large-scale CT image-report datasets:</p>
<ul>
<li>[cite_start]<strong>AMOS-MM 2024</strong> [cite: 80]</li>
<li>[cite_start]<strong>CT-Rate</strong> [cite: 82]</li>
<li>[cite_start]<strong>Abdomen Atlas 3.0</strong> [cite: 84]</li>
</ul>
<p>[cite_start]Additionally, we expanded the dataset by using GPT-4o mini to rewrite reports and generate clinically relevant question-answer pairs, enriching the data&#39;s diversity[cite: 86, 87].</p>
<hr>
<h2 id="-implementation-details">üõ†Ô∏è Implementation Details</h2>
<ul>
<li>[cite_start]<strong>Vision Encoder</strong>: 3D ViT from M3D-CLIP[cite: 89].</li>
<li>[cite_start]<strong>Base LLM</strong>: Llama-3.2-1B-Instruct[cite: 89].</li>
<li>[cite_start]<strong>Training</strong>: AdamW optimizer with warm-up and cosine decay, using bf16 mixed-precision training enabled by DeepSpeed on 4x NVIDIA A40 GPUs[cite: 90, 91].</li>
<li>[cite_start]<strong>Œº¬≤ Tokenizer Config</strong>: 4 Spatio-Temporal Attention Layers, 4 Text Condition Token Attention layers, 8 attention heads, and k=1024 for soft token selection[cite: 92].</li>
</ul>
<hr>
<h2 id="-citation">‚úíÔ∏è Citation</h2>
<p>If you find this work useful, please consider citing our paper:</p>
<pre><code class="lang-bibtex">@inproceedings{li2024mutokenizer,
  title={$\mu^{<span class="hljs-number">2</span>}$ Tokenizer: <span class="hljs-keyword">Differentiable </span><span class="hljs-keyword">Multi-Scale </span><span class="hljs-keyword">Multi-Modal </span>Tokenizer for Radiology Report Generation},
  author={Li, Siyou <span class="hljs-keyword">and </span>Qin, Pengyao <span class="hljs-keyword">and </span>Wu, Huanan <span class="hljs-keyword">and </span>Nie, Dong <span class="hljs-keyword">and </span>Thirunavukarasu, Arun <span class="hljs-keyword">J. </span><span class="hljs-keyword">and </span>Yu, <span class="hljs-keyword">Juntao </span><span class="hljs-keyword">and </span>Zhang, Le},
  year={<span class="hljs-number">2024</span>},
  publisher={Springer}
}
</code></pre>
<hr>
<h2 id="-contact">üìß Contact</h2>
<p>For any questions, please feel free to contact:</p>
<ul>
<li>[cite_start]Siyou Li: <code>siyou.li@qmul.ac.uk</code> [cite: 2]</li>
<li>[cite_start]Juntao Yu: <code>juntao.yu@qmul.ac.uk</code> [cite: 2]</li>
<li>[cite_start]Le Zhang: <code>l.zhang.16@bham.ac.uk</code> [cite: 2]</li>
</ul>

    <div class="w-full max-w-6xl mx-auto p-4 mb-8">
        <div class="p-4 bg-base-200 rounded-lg">
            <div class="relative">
                <textarea id="citation-text" readonly class="textarea textarea-bordered w-full bg-base-100 pr-12"
                    rows="3">@article{li2024mutokenizer,
  title={Œº¬≤Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation},
  author={Li, Siyou and Qin, Pengyao and Wu, Huanan and Nie, Dong and Thirunavukarasu, Arun J. and Yu, Juntao and Zhang, Le},
  year={2024}
}</textarea>
                <button id="copy-button" class="btn btn-sm btn-ghost absolute top-2 right-2"
                    data-clipboard-target="#citation-text">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4" fill="none" viewBox="0 0 24 24"
                        stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2"
                            d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                    </svg>
                </button>
            </div>
        </div>
    </div>

    <script>
        function toggleTheme() {
            const html = document.documentElement;
            const btn = document.getElementById('theme-toggle');
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            html.setAttribute('data-theme', newTheme);
            btn.textContent = newTheme === 'dark' ? '‚òÄÔ∏è' : 'üåô';
        }

        // Êñ∞ÁöÑÂ§çÂà∂ÂäüËÉΩÂÆûÁé∞
        document.getElementById('copy-button').addEventListener('click', async function () {
            const text = document.getElementById('citation-text').value;
            try {
                await navigator.clipboard.writeText(text);

                // Êõ¥Êñ∞ÊåâÈíÆÂõæÊ†á‰∏∫ÊàêÂäüÁä∂ÊÄÅ
                const btn = this;
                const originalHTML = btn.innerHTML;
                btn.innerHTML = `<svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
          </svg>`;

                setTimeout(function () {
                    btn.innerHTML = originalHTML;
                }, 2000);
            } catch (err) {
                console.error('Failed to copy text: ', err);
            }
        });
    </script>
    <script type="module" async>
        import { Niivue } from "https://unpkg.com/@niivue/niivue@0.57.0/dist/index.js"
        var volumeList = [
            { url: "static/images/amos_5001.nii.gz" },
        ];
        var nv = new Niivue({ isResizeCanvas: true });
        await nv.attachTo("gl");
        await nv.loadVolumes(volumeList);
    </script>
</body>

</html>