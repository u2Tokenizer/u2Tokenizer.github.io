<!DOCTYPE html>
<html lang="zh" data-theme="dark">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>‰∏≠Ëã±Êñá Tab ÂàáÊç¢Á§∫‰æã</title>
    <script>
        tailwind = {}
        tailwind.config = {
            theme: {
                extend: {},
            },
            plugins: [window.daisyui],
            daisyui: {
                themes: ["dark", "light"],
            },
        }
    </script>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdn.jsdelivr.net/npm/daisyui@4.7.2/dist/full.css" rel="stylesheet" type="text/css" />
    <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
</head>

<body class="min-h-screen flex flex-col items-center">
    <button id="theme-toggle" onclick="toggleTheme()" class="absolute right-0 top-0 mt-2 mr-2 btn btn-sm">Ô∏èÔ∏èÔ∏èÔ∏èÔ∏èÔ∏è‚òÄ</button>
    <div class="text-center max-w-3xl px-4">
        <h1 class="text-3xl font-bold my-8"><var>&micro<sup>2</sup></var>Tokenizer: Differentiable Multi-Scale
            Multi-Modal Tokenizer for Radiology Report Generation</h1>
        <div class="flex flex-wrap justify-center gap-2 text-lg mb-2">
            <span>Siyou Li<sup>1</sup>, </span>
            <span>Pengyao Qin<sup>2</sup>, </span>
            <span>Huanan Wu<sup>3</sup>, </span>
            <span>Dong Nie<sup>4</sup>, </span>
            <span>Arun J. Thirunavukarasu<sup>5</sup>, </span>
            <span>Juntao Yu<sup>1</sup>, </span>
            <span>Le Zhang<sup>2, 6</sup></span>
        </div>
        <div class="text-base text-base-content/80">
            <p><sup>1</sup>School of Electronic Engineering and Computer Science,</p>
            <p>Queen Mary University of London, London, UK</p>
            <p><sup>2</sup>School of Engineering, College of Engineering and Physical Sciences,</p>
            <p>University of Birmingham, Birmingham, UK</p>
            <p><sup>3</sup>Guangdong University of Technology, Guangdong, China</p>
            <p><sup>4</sup>Meta Inc. US</p>
            <p><sup>5</sup>Nuffield Department of Clinical Neurosciences,</p>
            <p>University of Oxford, Oxford, UK</p>
            <p><sup>6</sup>William Harvey Research Institute, NIHR Barts Biomedical Research Centre,</p>
            <p>Queen Mary University London, London, UK</p>
        </div>
    </div>
    
    <section>
        <h1>Introduction</h1>
        
        <p>
            Radiology reports are the primary medium through which radiologists communicate critical findings, diagnoses, and management recommendations to referring physicians. The accuracy and interpretability of these reports are paramount, especially for complex modalities like Computed Tomography (CT), as ambiguities or errors can lead to clinical missteps and increased patient anxiety. An increasing volume of CT examinations, compounded by radiologist workforce shortages, has created significant pressure to improve the efficiency of the reporting process. Emerging artificial intelligence (AI) and natural language processing (NLP) technologies show immense promise in automating Radiology Report Generation (RRG), which may streamline radiologist workflows, reduce reporting time, and enhance overall report quality. If successful, automated RRG could also facilitate large-scale data extraction for clinical research, thereby improving the utility of radiological data and ultimately enhancing diagnostic accuracy and patient outcomes.
        </p>

        <p>
            However, progress in automated RRG is hindered by two key challenges. First, there is an inherent complexity in extracting relevant information from high-dimensional imaging data under strict computational resource constraints. Existing models often resize CT images to fixed dimensions, a process that can distort anatomical details and erase subtle lesions, thereby compromising diagnostic accuracy. The direct use of high-resolution CT volumes is frequently prohibited by limited computational power, making the efficient extraction of pertinent imaging data a critical bottleneck. Second, there is a fundamental difficulty in objectively evaluating the clinical quality of generated reports. Traditional NLP metrics like BLEU and ROUGE focus on lexical similarity rather than clinical salience. As clinicians prioritize the content of findings over strict character-level alignment, these metrics are poorly suited for evaluating RRG systems, as they fail to capture the semantic and clinical relevance of generated reports compared to ground-truth references.
        </p>

        <p>
            To address these challenges, we propose <strong>Œº<sup>2</sup>LLM</strong>, a novel framework based on a multi-scale multimodal large language model designed specifically for RRG tasks. The central innovation of our framework is the <strong>Œº<sup>2</sup> Tokenizer</strong>, a differentiable multi-scale, multi-modal intermediate layer that efficiently bridges the vision and language domains. As illustrated in Figure 1, this tokenizer applies multi-level attention mechanisms and multi-scale aggregation to the visual tokens extracted from a 3D Vision Transformer (ViT3D). It then seamlessly fuses these refined visual embeddings with input question embeddings, maximizing their semantic correspondence while maintaining computational efficiency. This approach allows our model to preserve critical imaging details from variable-sized CT scans without incurring prohibitive computational costs.
        </p>

        <figure>
            <img src="placeholder_figure_1.png" alt="Overview of the proposed Œº¬≤LLM model architecture" style="width:100%; max-width:600px; border: 1px solid #ccc;">
            <figcaption>
                <strong>Figure 1:</strong> An overview of our proposed Œº<sup>2</sup>LLM model, centered on the Œº<sup>2</sup> Tokenizer layer for high-quality Radiology Report Generation (RRG). The model processes a CT image and a textual question, fuses them via the tokenizer, and generates a detailed report using a large language model.
            </figcaption>
        </figure>

        <p>
            Furthermore, we introduce a new paradigm for model evaluation and optimization to overcome the limitations of traditional metrics. We employ the <strong>GREEN model</strong>, a specialized RRG metric that leverages large language model-based natural language understanding to identify clinically significant errors and provide a nuanced evaluation of report quality. To align our model's outputs with expert-validated clinical accuracy, we utilize <strong>Direct Preference Optimization (DPO)</strong>. By using GREEN scores to create a preference dataset of "winning" and "losing" reports, we fine-tune Œº<sup>2</sup>LLM to generate responses that are not just lexically similar but are semantically and clinically superior.
        </p>

        <p>
            Our main contributions are threefold:
        </p>
        <ol class="contribution-list">
            <li>We propose a novel automated RRG framework, Œº<sup>2</sup>LLM, with its core Œº<sup>2</sup> Tokenizer, designed to efficiently preserve critical imaging details from 3D CT scans.</li>
            <li>We introduce a new training methodology that leverages the clinically-aware GREEN metric and Direct Preference Optimization (DPO) to align model outputs with expert standards of accuracy.</li>
            <li>We demonstrate through comprehensive experiments on three large-scale CT image-report datasets that our approach produces clinically salient reports and significantly outperforms existing, often much larger, baseline models.</li>
        </ol>
        <p>
            The remainder of this paper is organized as follows: Section 2 details our proposed method, Section 3 presents our experimental setup and results, and Section 4 concludes with a summary of our findings.
        </p>

    </section>

    <div class="w-full max-w-6xl mx-auto p-4 mb-8">
        <div class="p-4 bg-base-200 rounded-lg">
            <div class="relative">
                <textarea id="citation-text" readonly class="textarea textarea-bordered w-full bg-base-100 pr-12"
                    rows="3">@article{li2024mutokenizer,
  title={Œº¬≤Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation},
  author={Li, Siyou and Qin, Pengyao and Wu, Huanan and Nie, Dong and Thirunavukarasu, Arun J. and Yu, Juntao and Zhang, Le},
  year={2024}
}</textarea>
                <button id="copy-button" class="btn btn-sm btn-ghost absolute top-2 right-2"
                    data-clipboard-target="#citation-text">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4" fill="none" viewBox="0 0 24 24"
                        stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2"
                            d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                    </svg>
                </button>
            </div>
        </div>
    </div>

    <script>
        function toggleTheme() {
            const html = document.documentElement;
            const btn = document.getElementById('theme-toggle');
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            html.setAttribute('data-theme', newTheme);
            btn.textContent = newTheme === 'dark' ? '‚òÄÔ∏è' : 'üåô';
        }

        // Êñ∞ÁöÑÂ§çÂà∂ÂäüËÉΩÂÆûÁé∞
        document.getElementById('copy-button').addEventListener('click', async function () {
            const text = document.getElementById('citation-text').value;
            try {
                await navigator.clipboard.writeText(text);

                // Êõ¥Êñ∞ÊåâÈíÆÂõæÊ†á‰∏∫ÊàêÂäüÁä∂ÊÄÅ
                const btn = this;
                const originalHTML = btn.innerHTML;
                btn.innerHTML = `<svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
          </svg>`;

                setTimeout(function () {
                    btn.innerHTML = originalHTML;
                }, 2000);
            } catch (err) {
                console.error('Failed to copy text: ', err);
            }
        });
    </script>
    <script type="module" async>
        import { Niivue } from "https://unpkg.com/@niivue/niivue@0.57.0/dist/index.js"
        var volumeList = [
            { url: "static/images/amos_5001.nii.gz" },
        ];
        var nv = new Niivue({ isResizeCanvas: true });
        await nv.attachTo("gl");
        await nv.loadVolumes(volumeList);
    </script>
</body>

</html>